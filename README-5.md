# CS5720 Neural Networks and Deep Learning - Home Assignment 5

**Student Name:** Satwika Kallem

**University:** University of Central Missouri

**Department:** Computer Science & Cybersecurity

**Course:** CS5720 Neural Networks and Deep Learning

**Semester:** Spring 2025




## 1. GAN Architecture

**Adversarial Process in GAN Training:**

The adversarial process in Generative Adversarial Networks (GANs) involves a game between two neural networks: the **Generator (G)** and the **Discriminator (D)**. These networks compete against each other in a zero-sum game.

* **Generator's Goal:** The generator aims to learn the underlying distribution of the real training data and generate new synthetic data samples that are indistinguishable from real data. It tries to "fool" the discriminator into believing its generated samples are real.

* **Discriminator's Goal:** The discriminator acts as a binary classifier. Its objective is to distinguish between real data samples (drawn from the training dataset) and fake data samples produced by the generator. It tries to correctly identify whether an input sample is real or fake.

**Improvement Through Competition:**

The generator and discriminator are trained simultaneously in an iterative process:

1.  The **discriminator** is trained on a batch of real data labeled as "real" and a batch of fake data generated by the generator labeled as "fake." The discriminator updates its weights to improve its ability to correctly classify both types of samples.

2.  The **generator** is then trained to produce samples that the discriminator will classify as "real." The generator uses the discriminator's feedback (gradients) to adjust its weights in a way that increases the likelihood of its generated samples being classified as real.

This competitive process drives both networks to improve:

* The **generator** learns to create increasingly realistic synthetic data to better fool the discriminator.
* The **discriminator** becomes more adept at distinguishing subtle differences between real and fake data as the generator improves.

Ideally, this process reaches an equilibrium where the generator produces samples that are statistically indistinguishable from the real data, and the discriminator can no longer confidently differentiate between them (its accuracy approaches 50%).

**Objectives:**

* **Generator (G):** Minimize the probability that the discriminator correctly identifies its output as fake (i.e., $min_G \log(1 - D(G(z)))$).
* **Discriminator (D):** Maximize the probability of correctly classifying real data as real and fake data as fake (i.e., $max_D [\log(D(x)) + \log(1 - D(G(z)))]$).

## 2. Ethics and AI Harm

**Chosen AI Harm:** Representational harm

**Real or Hypothetical Application:** AI-powered facial recognition systems used for security access control in a diverse workplace.

**Description of the Harm:** Representational harm occurs when AI systems perpetuate or amplify societal biases by underrepresenting or misrepresenting certain demographic groups. In this facial recognition application, if the training data used to develop the system disproportionately features certain ethnicities or genders and lacks sufficient representation of others, the system might exhibit lower accuracy in recognizing individuals from the underrepresented groups. This can lead to:

* **Denial of Access:** Employees from underrepresented groups might experience more frequent failures in accessing secure areas, causing inconvenience, delays, and potentially hindering their work.
* **Systemic Bias Reinforcement:** The deployment of such a biased system can reinforce existing societal biases by implicitly suggesting that the faces of certain groups are less "recognizable" or less "valid" for identification.
* **Erosion of Trust:** Individuals from the affected groups may lose trust in the fairness and reliability of the AI system and the organization deploying it.

**Harm Mitigation Strategies:**

Based on the lecture, here are two harm mitigation strategies:

1.  **Data Diversity and Augmentation:**
    * **Action:** Ensure the training dataset used for the facial recognition system is diverse and representative of the actual workplace demographics. This involves actively collecting data that includes a balanced representation of different ethnicities, genders, age groups, and skin tones.
    * **Mechanism:** By training the model on a more inclusive dataset, the AI can learn to recognize facial features across various demographic groups with greater accuracy and reduce disparities in performance. Data augmentation techniques (e.g., applying transformations like slight rotations, changes in lighting) can further increase the robustness and generalization of the model to diverse faces.

2.  **Bias Auditing and Fairness Metrics:**
    * **Action:** Implement rigorous bias auditing processes throughout the development and deployment lifecycle of the facial recognition system. Utilize fairness metrics relevant to recognition accuracy across different groups, such as **Equal Opportunity Difference** (difference in true positive rates) or **Statistical Parity Difference** (difference in acceptance rates).
    * **Mechanism:** Regularly evaluate the model's performance on diverse test datasets, calculating these fairness metrics to identify potential disparities. If significant bias is detected, developers can iterate on the model architecture, training process, or data to mitigate these issues. Tools like Aequitas (as discussed in Question 6) can be invaluable for this auditing process. Setting acceptable thresholds for bias metrics can ensure the system meets fairness standards before deployment.
